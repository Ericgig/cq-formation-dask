{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e5a8e9e-fd2e-4dc8-aaa0-cf760e16d31d",
   "metadata": {},
   "source": [
    "Dask\n",
    "====\n",
    "Parallel Python  \n",
    "Fast and Easy\n",
    "-----------------\n",
    "\n",
    "`dask` is a module to spread computing tasks and data accross multiple workers.  \n",
    "\n",
    "It support support both local and distributed parallelism.\n",
    "\n",
    "Tasks can be split using:\n",
    "- Futures\n",
    "- **Delayed**\n",
    "\n",
    "And data using:\n",
    "- **Array**\n",
    "- Daraframe\n",
    "- Bags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1810f59-4552-4348-852c-3ffadc594e18",
   "metadata": {},
   "source": [
    "Why dask?\n",
    "---------\n",
    "\n",
    "1- **Familiarity**:\n",
    "\n",
    "    Dask want to make it as easy as possible to scale conputation, so it mostly follows other popular project api:\n",
    "    \n",
    "    - dask.array ~= numpy, Xarray  \n",
    "    - dask.dataframe ~= pandas  \n",
    "    - dask's future ~ multiprocessing  \n",
    "\n",
    "2- **Scalable**:\n",
    "\n",
    "    Dask can be used on a local machine, on a cluster and in the cloud.  \n",
    "    It synchornize works from as much ressource as you need.\n",
    "\n",
    "When to use dask?\n",
    "-----------------\n",
    "\n",
    "- When datasets get too larges to fit locally.  \n",
    "Use dask only when needed. Locally, numpy, numba, cupy, jax etc. can usually do a better job!\n",
    "\n",
    "But when the scale is such that it doesn't fit in memory locally, then it's worth looking at dask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2b16dd-3684-43f6-a209-0e96f547c3c0",
   "metadata": {},
   "source": [
    "Useful links\n",
    "------------\n",
    "- [Documentation](https://docs.dask.org/en/stable/)\n",
    "- [Dask-Cookbook](https://projectpythia.org/dask-cookbook/notebooks/00-dask-overview.html)\n",
    "- [github](https://github.com/dask/dask/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d029386-d726-49b1-b786-75a47a9ec26f",
   "metadata": {},
   "source": [
    "First let's install dask and related modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc096354-33b0-4089-8a66-202587006427",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install dask[array,distributed,diagnostics] graphviz sparse scipy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c99a303-9767-4680-a44b-566f535c14c2",
   "metadata": {},
   "source": [
    "When using dask, you submit your workload from the client. Then the scheduler split that workload unto workers:\n",
    "\n",
    "\n",
    "![Dask client](https://tutorial.dask.org/_images/distributed-overview.png)\n",
    "[_Image from the dask-cookbook of Project Pythia_](https://projectpythia.org/dask-cookbook/notebooks/00-dask-overview.html)\n",
    "\n",
    "\n",
    "Using dask starts by starting the `Client`.\n",
    "\n",
    "Once created, the client is used automatically by all following dask jobs.  \n",
    "We can also dispatch work to the client directly, somewhat similarly to multiprocessing pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721877d4-a00b-42cf-958e-b598eef370db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d743928-68aa-421b-927e-453d206f57dd",
   "metadata": {},
   "source": [
    "Now let's use that client.  \n",
    "We can ask it to send a job to workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dccfaf3-7319-4596-acbd-27b4dc2b2804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mul(a, b):\n",
    "    return a * b\n",
    "\n",
    "future1 = client.submit(mul, 11, 10)\n",
    "future2 = client.submit(mul, 4, 4)\n",
    "future1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4030cc12-f55f-412e-8942-9b61f532d401",
   "metadata": {},
   "source": [
    "The task submitted, it's waiting for a worker to be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8843766b-330d-4f67-89cb-02248aa01794",
   "metadata": {},
   "outputs": [],
   "source": [
    "future1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c5924a-71ee-44c1-84e7-58358c227c9b",
   "metadata": {},
   "source": [
    "When finished, the `result` method is needed to get the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e17604-a698-4fbb-b808-b322d1f15592",
   "metadata": {},
   "outputs": [],
   "source": [
    "future1.result(), future2.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3d8c99-5a48-4c5d-92c3-5b8042fceff9",
   "metadata": {},
   "source": [
    "To run multiple tasks at once, we can call them all in a loop or use `map`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cc4d0c-ea18-4b36-8f18-bf80ae364436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double(x):\n",
    "    return x * 2\n",
    "\n",
    "N = 10\n",
    "futures = client.map(double, range(N))\n",
    "futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d795c8-c19b-451b-a139-e0d5b546727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[future.result() for future in futures]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefddb01-dc13-4b19-a7e0-c7219c0ffa3a",
   "metadata": {},
   "source": [
    "## Using a cluster\n",
    "\n",
    "Dask support multiple clusters types:\n",
    "- Local\n",
    "  - threading\n",
    "  - process\n",
    "- Distributed\n",
    "  - HPC (dask_jobqueue)\n",
    "    - PBS\n",
    "    - Slurm\n",
    "    - ...\n",
    "  - Cloud (dask_cloudprovider)\n",
    "    - Azure\n",
    "    - AWS\n",
    "    - ...\n",
    "  - Coiled\n",
    " \n",
    "### Only the configuration step changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f4cd6a-4adc-4332-9e52-2ec474156c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon\n",
    "try:\n",
    "    from dask_cloudprovider.aws import FargateCluster\n",
    "    cluster = FargateCluster(\n",
    "        # Cluster manager specific config kwargs\n",
    "    )\n",
    "    client = Client(cluster)\n",
    "    ...\n",
    "    cluster.close()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39686577-95ad-4e55-8984-439969a87ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLURM\n",
    "try:\n",
    "    from dask_jobqueue.slurm import SLURMRunner\n",
    "    with SLURMRunner() as cluster:\n",
    "        with Client(cluster) as client:\n",
    "            # Wait for all the workers to be ready before continuing.\n",
    "            client.wait_for_workers(runner.n_workers)\n",
    "            main()\n",
    "\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce67334e-52d3-421d-971d-304cdd033431",
   "metadata": {},
   "source": [
    "On the Alliance server, you should use `SLURMRunner` not ~`SLURMCluster`~.  \n",
    "- `SLURMRunner`: Create a worker for each task in a slurm job. Good for cluster that prefer fewer larger jobs.\n",
    "- `SLURMCluster`: Uses a different slurm job for each workers, which all need to go through the waiting queue...\n",
    "  Good for cluster that prefer lot of small jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ed7ec1-8e5e-48ae-981e-8f0cb05bb1a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dask",
   "language": "python",
   "name": "dask"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
